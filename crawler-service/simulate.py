import asyncio
import base64
import json
import logging
import sys
import time
import uuid
from pathlib import Path

# Add the crawler directory to path
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from app.auth_manager import auth_manager
from app.session_store import session_store
from app.adapters.xiaohongshu_adapter import XiaohongshuAdapter
from app.risk_control import RiskController
from app.config import settings
from app.models import CrawlerJobPayload, CrawlerJobLimits

async def main():
    # å¼ºåˆ¶å…³é—­æ‰€æœ‰çš„ä»£ç†é…ç½®ï¼ˆç›´è¿ï¼‰å¹¶æ‰“å¼€æµè§ˆå™¨çœ‹åˆ°ç•Œé¢
    settings.crawler_default_proxy_server = ""
    settings.crawler_default_proxy_username = ""
    settings.crawler_default_proxy_password = ""
    settings.crawler_playwright_headless = False

    print("\nã€æ³¨æ„ã€‘ç”±äºä»£ç†IPæ± ç™½åå•æ— æ³•ä½¿ç”¨å½“å‰IPï¼Œæˆ‘ä»¬å°†ä¸èµ°äºŒæ¬¡ä»£ç†ï¼Œä½¿ç”¨æœ¬æœºç½‘ç»œç›´è¿å°çº¢ä¹¦ã€‚")
    print(">>> è¿™ä¼šåœ¨åå°å¯åŠ¨ä¸€ä¸ªã€æœ‰å¤´æµè§ˆå™¨ã€‘çª—å£ä»¥æ–¹ä¾¿æ’æŸ¥ç½‘ç»œç¯å¢ƒã€‚")
    
    platform = "xiaohongshu"
    user_id = "test-user-" + uuid.uuid4().hex[:8]
    
    print("\n>>> 1. æ­£åœ¨è·å–å°çº¢ä¹¦ç™»å½•äºŒç»´ç ...")
    start = await auth_manager.start_flow(platform=platform, user_id=user_id, region="")
    flow_id = start.get("flow_id")
    qr = start.get("qr_image_base64")
    
    if not flow_id or not qr:
        print("âŒ è·å–äºŒç»´ç å¤±è´¥:", start)
        return

    qr_path = ROOT.parent / ".tmp-ref" / "qr-auth" / "simulate.png"
    qr_path.parent.mkdir(parents=True, exist_ok=True)
    qr_path.write_bytes(base64.b64decode(qr))
    
    import platform as sys_platform
    import subprocess
    system = sys_platform.system().lower()
    if "darwin" in system:
        subprocess.Popen(["open", str(qr_path)])
    
    print(f"\n[æ‰«ç æç¤º] è¯·ä½¿ç”¨å°çº¢ä¹¦ App æ‰«æå·²å¼¹å‡ºçš„äºŒç»´ç ç¡®è®¤ç™»å½•ï¼(Flow ID: {flow_id})")
    print(">>> 2. ç­‰å¾…æˆæƒè¿”å›ç»“æœ (é™æ—¶ 180 ç§’)...\n")
    
    deadline = time.time() + 180
    authorized = False
    while time.time() < deadline:
        status = await auth_manager.get_status(flow_id)
        state = status.get("status")
        if state == "authorized":
            print("\nâœ… æˆæƒæˆåŠŸï¼")
            authorized = True
            break
        elif state in ("failed", "expired", "cancelled"):
            print(f"\nâŒ æˆæƒæµç¨‹å¼‚å¸¸ä¸­æ­¢: {state}")
            return
        await asyncio.sleep(2)
        print(".", end="", flush=True)

    if not authorized:
        print("\nâŒ æ‰«ç è¶…æ—¶ï¼")
        return
        
    print("\n>>> 3. åˆ†æéœ€æ±‚å¹¶æ‹†è§£æˆå…³é”®è¯...")
    idea = "è¾“å…¥ä»»æ„ä¸¥è‚ƒçš„ä¸»é¢˜ï¼ŒAI ä¼šè¿ç”¨è’è¯é€»è¾‘å’Œåç›´è§‰çš„ç±»æ¯”ï¼Œç”Ÿæˆä¸€æ®µçœ‹ä¼¼æ·±å¥¥å®åˆ™æ¯«æ— æ„ä¹‰çš„â€œåºŸè¯â€æ–‡å­¦æˆ–è„±å£ç§€æ®µå­ã€‚"
    print(f"åŸå§‹éœ€æ±‚: {idea}")
    keywords = ["åºŸè¯æ–‡å­¦", "AIè„±å£ç§€æ®µå­", "åè§’ç±»æ¯”", "è’è¯é€»è¾‘"]
    print(f"æ‹†è§£å‡ºçš„é•¿å°¾/æ ¸å¿ƒæœç´¢å…³é”®è¯: {keywords}\n")
    
    print(">>> 4. å¼€å§‹å¹¶è¡ŒæŠ“å–å°çº¢ä¹¦å¸–å­å’Œè¯„è®º (ä½¿ç”¨ç›´è¿)...")
    risk = RiskController(session_pool_size=1, user_agent_pool=settings.crawler_user_agent_pool)
    adapter = XiaohongshuAdapter(risk)
    
    for kw in keywords:
        print(f"\n==============================================")
        print(f"æœç´¢å…³é”®è¯: ã€{kw}ã€‘")
        print(f"==============================================")
        
        payload = CrawlerJobPayload(
            validation_id=f"sim-{uuid.uuid4().hex[:8]}",
            trace_id=f"sim-{uuid.uuid4().hex[:8]}",
            user_id=user_id,
            query=kw,
            platforms=[platform],
            mode="quick",
            limits=CrawlerJobLimits(notes=4, comments_per_note=6),
            freshness_days=14,
            timeout_ms=45000,
        )
        
        try:
            result, cost = await adapter.crawl(payload)
            
            if not result.success:
                print(f"âŒ æŠ“å–æŠ¥é”™: {result.error}")
                continue
                
            print(f"âœ… è·å–åˆ° {len(result.notes)} ç¯‡æœ‰æ•ˆç¬”è®° å’Œ {len(result.comments)} æ¡è¯„è®ºã€‚\n")
            
            # æ‰“å°éƒ¨åˆ†ç¬”è®°
            for i, note in enumerate(result.notes[:3]):
                print(f"ğŸ“ ç¬”è®° {i+1}: ã€Š{note.title}ã€‹")
                desc = note.desc[:150] + "..." if len(note.desc) > 150 else note.desc
                desc = desc.replace('\\n', ' ')
                print(f"   æ‘˜è¦: {desc}")
                print(f"   æ•°æ®: â¤ï¸ {note.liked_count}ç‚¹èµ | ğŸ’¬ {note.comments_count}è¯„è®º | â­ {note.collected_count}æ”¶è—\n")
            
            # æ‰“å°éƒ¨åˆ†è¯„è®º
            print(f"ğŸ’¬ ç›¸å…³ä»£è¡¨æ€§ç”¨æˆ·è¯„è®ºæŠ½æ ·:")
            comment_samples = [c for c in result.comments if c.content]
            for i, c in enumerate(comment_samples[:4]):
                content = c.content[:100].replace('\\n', ' ')
                print(f"   [{c.user_nickname}]: {content}")
        except Exception as e:
            print(f"âŒ æŠ“å–å…³é”®è¯ {kw} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

    print("\n\n>>> æ¨¡æ‹ŸéªŒè¯æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")

if __name__ == "__main__":
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("app.adapters.xiaohongshu_adapter").setLevel(logging.INFO)
    asyncio.run(main())
