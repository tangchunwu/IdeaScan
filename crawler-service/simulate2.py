import asyncio
import base64
import json
import logging
import sys
import time
import uuid
import os
from pathlib import Path

# Add the crawler directory to path
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from app.session_store import session_store
from app.adapters.xiaohongshu_adapter import XiaohongshuAdapter
from app.risk_control import RiskController
from app.config import settings
from app.models import CrawlerJobPayload, CrawlerJobLimits
from playwright.async_api import async_playwright

async def get_or_create_session(platform, user_id):
    print(">>> 1. æ­£åœ¨è°ƒèµ·å¸¦ç¼“å­˜çš„æµè§ˆå™¨çª—å£ (é˜²æ­¢é‡å¤æ‰«ç )...")
    profile_dir = ROOT.parent / ".tmp-ref" / "xhs_profile"
    profile_dir.mkdir(parents=True, exist_ok=True)
    
    async with async_playwright() as p:
        # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œæ‰«ä¸€æ¬¡ç ä»¥åå°±è®°ä½äº†
        context = await p.chromium.launch_persistent_context(
            user_data_dir=str(profile_dir),
            headless=False,
            args=["--disable-blink-features=AutomationControlled"],
            viewport={"width": 1280, "height": 800}
        )
        page = context.pages[0] if context.pages else await context.new_page()
        print("ğŸŒ æ­£åœ¨è®¿é—®å°çº¢ä¹¦ç½‘é¡µç«¯...")
        await page.goto("https://www.xiaohongshu.com/explore", wait_until="domcontentloaded")
        
        print("\n=======================================================")
        print("ğŸ•µï¸â€â™‚ï¸ çŠ¶æ€æ£€æµ‹ä¸­...è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­ç¡®è®¤æ‚¨æ˜¯å¦å·²å¤„äºã€ç™»å½•çŠ¶æ€ã€‘ã€‚")
        print("å¦‚æœæœªç™»å½•ï¼Œè¯·åœ¨æµè§ˆå™¨é‡Œç›´æ¥æ‰«ç ç™»å½•ã€‚")
        print("=======================================================\n")
        
        # ç­‰å¾…é¡µé¢å‡ºç°ç™»å½•åçš„ç‰¹å¾ï¼ˆå¦‚ä¾§è¾¹æ çš„ç”¨æˆ·å¤´åƒæˆ–ç”¨æˆ·åç§°ï¼‰
        logged_in = False
        for _ in range(30):
            try:
                # å°çº¢ä¹¦ç™»å½•åé€šå¸¸ä¾§è¾¹æ ä¼šæœ‰ç”¨æˆ·ä¿¡æ¯æˆ–è€…å‘ç¬”è®°æŒ‰é’®
                is_avatar_visible = await page.locator("a.user-profile").is_visible()
                is_create_visible = await page.locator(".publish-btn").is_visible()
                if is_avatar_visible or is_create_visible:
                    logged_in = True
                    break
            except Exception:
                pass
            await asyncio.sleep(2)
            
        if not logged_in:
             print("âš ï¸ æœªè‡ªåŠ¨æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼Œç­‰å¾…é¢å¤– 30 ç§’ä¾›æ‚¨æ‰«ç ...")
             await asyncio.sleep(30)
             
        print("âœ… ç¡®è®¤ç™»å½•ç¯å¢ƒå°±ç»ªï¼æ­£åœ¨æå–å¹¶ä¿å­˜å®‰å…¨ Cookie...")
        cookies = await context.cookies()
        
        # å°† Cookie å†™å…¥çˆ¬è™«ä¼šè¯æ± 
        await session_store.upsert_user_session(
            platform=platform,
            user_id=user_id,
            cookies=cookies,
            user_agent=settings.crawler_user_agent_pool.split(",")[0].strip(),
            region="",
            source="manual_persistent_login",
        )
        await context.close()

async def main():
    # å¼ºåˆ¶ç›´è¿
    settings.crawler_default_proxy_server = ""
    settings.crawler_default_proxy_username = ""
    settings.crawler_default_proxy_password = ""
    settings.crawler_playwright_headless = False
    
    # æ ¸å¿ƒï¼šé™ä½çˆ¬è™«é¢‘ç‡ï¼Œä¿æŠ¤è´¦å· (å»¶è¿Ÿè°ƒé«˜è‡³ 4-7 ç§’ï¼)
    settings.crawler_quick_delay_ms_min = 4000
    settings.crawler_quick_delay_ms_max = 7000
    settings.crawler_deep_delay_ms_min = 4000
    settings.crawler_deep_delay_ms_max = 7000

    platform = "xiaohongshu"
    user_id = "test-user-safe"
    
    await get_or_create_session(platform, user_id)
        
    print("\n>>> 2. åˆ†æéœ€æ±‚å¹¶æ‹†è§£æˆå…³é”®è¯...")
    idea = "AI è¾…åŠ©å†™è’è¯é€»è¾‘å’Œåç›´è§‰ç±»æ¯”çš„åºŸè¯æ–‡å­¦å’Œè„±å£ç§€"
    print(f"åŸå§‹éœ€æ±‚: {idea}")
    keywords = ["AIåºŸè¯æ–‡å­¦", "AIè„±å£ç§€æ®µå­", "åç›´è§‰ç±»æ¯”"]
    print(f"å®‰å…¨æŠ“å–ç­–ç•¥å·²å¼€å¯ (æ¯æ¬¡æ“ä½œé—´éš” 4-7 ç§’)ã€‚æœç´¢å…³é”®è¯: {keywords}\n")
    
    print(">>> 3. å¼€å§‹æ‰§è¡Œç¼“æ…¢ä¸”å®‰å…¨çš„å¹¶è¡ŒæŠ“å–...")
    risk = RiskController(session_pool_size=1, user_agent_pool=settings.crawler_user_agent_pool)
    adapter = XiaohongshuAdapter(risk)
    
    for kw in keywords:
        print(f"\n==============================================")
        print(f"ğŸ” æ­£åœ¨å®‰å…¨æœç´¢å…³é”®è¯: ã€{kw}ã€‘")
        print(f"==============================================")
        
        # æ¯ä¸ªè¯åªé‡‡æ · 3 ç¯‡ç¬”è®°ï¼Œä¸”æ¯ç¯‡ç¬”è®°æœ€å¤šæ‹‰ 4 æ¡è¯„è®ºï¼Œç¡®ä¿ä¸è¿‡åº¦è¯·æ±‚
        payload = CrawlerJobPayload(
            validation_id=f"sim-{uuid.uuid4().hex[:8]}",
            trace_id=f"sim-{uuid.uuid4().hex[:8]}",
            user_id=user_id,
            query=kw,
            platforms=[platform],
            mode="quick",
            limits=CrawlerJobLimits(notes=3, comments_per_note=4),
            freshness_days=30,
            timeout_ms=60000, 
            ignore_relevance_filter=True,  # å…³é—­ä¸¥æ ¼çš„ç›¸å…³æ€§è¿‡æ»¤ä»¥ç¡®ä¿å°‘æ ·æœ¬æ—¶ä¹Ÿèƒ½åå‡ºæ•°æ®
        )
        
        try:
            result, cost = await adapter.crawl(payload)
            
            if not result.success:
                print(f"âŒ æŠ“å–æŠ¥é”™ (å¯èƒ½è§¦å‘äº†æ»‘å—éªŒè¯æˆ–é™æµ): {result.error}")
                # é‡åˆ°é£æ§ä¼‘æ¯ä¸€ä¸‹
                await asyncio.sleep(10)
                continue
                
            print(f"âœ… å®‰å…¨æ‹‰å–å®Œæ¯•ï¼è·å–åˆ° {len(result.notes)} ç¯‡æœ‰æ•ˆç¬”è®° å’Œ {len(result.comments)} æ¡è¯„è®ºã€‚\n")
            
            for i, note in enumerate(result.notes[:2]):
                print(f"ğŸ“ ç¬”è®° {i+1}: ã€Š{note.title}ã€‹")
                desc = note.desc[:100] + "..." if len(note.desc) > 100 else note.desc
                desc = desc.replace('\\n', ' ')
                print(f"   æ‘˜è¦: {desc}")
                print(f"   æ•°æ®è¡¨ç°: â¤ï¸ {note.liked_count} | ğŸ’¬ {note.comments_count} | â­ {note.collected_count}\n")
            
            print(f"ğŸ’¬ ä»£è¡¨æ€§è¯„è®ºæŠ½æ · (AIåˆ†æç´ æ):")
            comment_samples = [c for c in result.comments if c.content]
            for i, c in enumerate(comment_samples[:3]):
                content = c.content[:80].replace('\\n', ' ')
                print(f"   - {c.user_nickname}: {content}")
                
        except Exception as e:
            print(f"âŒ æŠ“å–å…³é”®è¯ {kw} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            
        print("\nâ³ å†·å´ç­‰å¾… 5 ç§’è¿›å…¥ä¸‹ä¸ªå…³é”®è¯...")
        await asyncio.sleep(5)

    print("\n\n>>> ğŸ‰ æ¨¡æ‹Ÿä»¿çœŸæ•°æ®è°ƒç ”å·²å®Œæˆï¼Œè´¦å·å®‰å…¨æ— è™ï¼")

if __name__ == "__main__":
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("app.adapters.xiaohongshu_adapter").setLevel(logging.INFO)
    asyncio.run(main())
